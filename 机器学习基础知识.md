# 数学基础
可查看  [【好文导读】机器学习相关的数学参考资料](https://mp.weixin.qq.com/s/F8LTUz4R4fbEa5EdVruiWA)，具体知识参考详细内容可参考 [DeepLearningBook.pdf](https://github.com/bettermorn/IAICourse/blob/main/refermaterials/DeepLearningBook.pdf)第2，3，4章
* 高等数学
* 线性代数
* 概率论与统计学
# 机器学习实践过程
## 数据探索
预处理过程包括
* https://www.kaggle.com/dansbecker/basic-data-exploration
* https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python
## 特征工程

## 经典机器学习方法
参考图片
### 监督学习
### 无监督学习
###
# 常见概念
## 理论知识
参考 [英伟达高级机器学习工程Aqeel Anwar撰写的18页精炼《机器学习面试速查表》机器学习中的关键要点]
(https://github.com/bettermorn/IAICourse/blob/main/refermaterials/ML_cheatsheets.pdf)
## 基本概念
基础知识可参考详细内容可参考 [DeepLearningBook.pdf](https://github.com/bettermorn/IAICourse/blob/main/refermaterials/DeepLearningBook.pdf)第5章
* 训练集、验证集和测试集
* 过拟合、欠拟合
* 交叉熵
* 预训练模型
* 正向传播，反向传播
* 特征工程：也是一种注意力机制的体现
## 深度学习
### 主要算法模型
图片链接 https://github.com/bettermorn/IAICourse/blob/main/img/deeplearnstructure.png
![常用深度学习](https://github.com/bettermorn/IAICourse/blob/main/img/deeplearnstructure.png)
* 全连接网络 FC
# 机器学习评估指标
参考  super-cheatsheet-machine-learning.pdf  https://github.com/bettermorn/IAICourse/blob/main/refermaterials/super-cheatsheet-machine-learning.pdf
# 机器学习训练的硬件和软件环境
## 硬件
 参考[典型深度神经网络模型训练服务器配置](https://github.com/bettermorn/IAICourse/wiki/%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B#%E5%85%B8%E5%9E%8B%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE)
## 软件
* 开源机器学习框架,如Pytorch，Tensorflow等
* 依赖包，如Python，R，Matlab等
* Python程序，C/C++程序等
# 机器学习训练方法和技巧
## 梯度算法的选择
数据稀疏，使用自适应方法，如Adagrad，Adadelta，RMSprop，Adam
## Critical Point: small gradient
## 选择损失函数Loss Function的方法
## Batch Normalization Batch and Momentum
## Adaptive Learning Rate optimizer
优化器用于调整初始的随机权重，并帮助模型更准确地计算目标值
学习率规划，图片链接  https://github.com/bettermorn/IAICourse/blob/main/img/LRS.png
![学习率规划](https://github.com/bettermorn/IAICourse/blob/main/img/LRS.png)
## 避免过拟合overfit的方法
## 需要调整的超参数
1. batch_size
2. 卷积核宽度
3. 神经网络的层数
4. 学习率 learning rate
决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。当学习率设置过小时，收敛过程将变得十分缓慢。学习率设置过大时，梯度可能会在最小值附近来回震荡，甚至可能无法收敛。常用学习率调整方法包括离散下降(discrete staircase)指数减缓(exponential decay)，详细内容可参考 [DeepLearningBook.pdf](https://github.com/bettermorn/IAICourse/blob/main/refermaterials/DeepLearningBook.pdf)4.3,8.2,8.5
5. Dropout比率
6. 隐藏单元数量
# Pytorch
* Pytorch参考文档 https://pytorch.org/docs/1.9.1/nn.html
* torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
* torch.nn.BatchNorm2d(num_features)
* torch.nn.ReLU()
* torch.nn.MaxPool2d(kernel_size, stride, padding)
* torch.nn.Linear(in_features, out_features)
* torch.nn.CrossEntropyLoss()
* torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)





