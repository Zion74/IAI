# 数学基础
可查看  [【好文导读】机器学习相关的数学参考资料](https://mp.weixin.qq.com/s/F8LTUz4R4fbEa5EdVruiWA)
* 高等数学
* 线性代数
* 概率论与统计学

# 常见概念
* 训练集、验证集和测试集
* 过拟合、欠拟合
* 交叉熵
* 预训练模型

# 机器学习评估指标
参考  super-cheatsheet-machine-learning.pdf  https://github.com/bettermorn/IAICourse/blob/main/refermaterials/super-cheatsheet-machine-learning.pdf
# 机器学习训练的硬件和软件环境

# 机器学习训练方法和技巧
## Critical Point: small gradient
## 选择损失函数Loss Function的方法
## Batch Normalization Batch and Momentum
## Adaptive Learning Rate optimizer
优化器用于调整初始的随机权重，并帮助模型更准确地计算目标值
## 避免过拟合overfit的方法
## 需要调整的超参数
1. batch_size
2. 卷积核宽度
3. 神经网络的层数
4. 学习率 learning rate
决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。当学习率设置过小时，收敛过程将变得十分缓慢。学习率设置过大时，梯度可能会在最小值附近来回震荡，甚至可能无法收敛。常用学习率调整方法包括离散下降(discrete staircase)指数减缓(exponential decay)，详细内容可参考 [DeepLearningBook.pdf](https://github.com/bettermorn/IAICourse/blob/main/refermaterials/DeepLearningBook.pdf)4.3,8.2,8.5
5. Dropout比率
6. 隐藏单元数量
# Pytorch
* Pytorch参考文档 https://pytorch.org/docs/1.9.1/nn.html
* torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
* torch.nn.BatchNorm2d(num_features)
* torch.nn.ReLU()
* torch.nn.MaxPool2d(kernel_size, stride, padding)
* torch.nn.Linear(in_features, out_features)
* torch.nn.CrossEntropyLoss()
* torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)





